<p align="center">
  <h1 align="center">ðŸ§¬ Intellectual DNA</h1>
  <p align="center">
    <strong>Turn 3 years of AI conversations into a queryable second brain</strong>
  </p>
  <p align="center">
    376K messages Â· 118K embeddings Â· 31 MCP tools Â· 256ms queries
  </p>
</p>

<p align="center">
  <a href="#"><img src="https://img.shields.io/badge/python-3.11+-blue?style=flat-square&logo=python&logoColor=white" alt="Python"></a>
  <a href="#"><img src="https://img.shields.io/badge/MCP-compatible-green?style=flat-square" alt="MCP"></a>
  <a href="#"><img src="https://img.shields.io/badge/vectors-LanceDB-orange?style=flat-square" alt="LanceDB"></a>
  <a href="#"><img src="https://img.shields.io/badge/embeddings-nomic--v1.5-purple?style=flat-square" alt="Embeddings"></a>
  <a href="https://github.com/mordechaipotash/intellectual-dna/stargazers"><img src="https://img.shields.io/github/stars/mordechaipotash/intellectual-dna?style=flat-square" alt="Stars"></a>
</p>

---

```
You: "What do I actually think about agency?"

Brain: Searching 118K embedded messages...

Your position evolved:
  2023: "AI should do what I say"
  2024: "AI should preserve my decision sovereignty"  
  2025: "100% human control, 100% machine execution"

Related SEED principle (AGENCY PRESERVATION):
"Maintain human decision-making control while automating everything else"
```

## What is this?

Every conversation you have with an AI is a thought you externalized. Over 3 years, that's **376,000 thoughts** â€” but they're scattered across ChatGPT exports, Claude sessions, Gemini chats, and code editor transcripts.

Intellectual DNA turns that scattered history into a **queryable knowledge system**. Not a note-taking app â€” a second brain that can:

- **Find patterns** you'd never think to search for
- **Track how your thinking evolved** on any topic
- **Surface contradictions** between what you say and what you do
- **Cross-reference** conversations with your GitHub commits, markdown docs, and more

It runs as an **MCP server** â€” plug it into Claude Code, Claude Desktop, or any MCP-compatible client, and your entire intellectual history becomes context.

## The Numbers

| Metric | Value |
|--------|-------|
| Conversation messages | **376,164** |
| Embedded vectors | **118,533** (768d, nomic-v1.5) |
| GitHub commits indexed | **2,217** across 146 repos |
| Markdown docs harvested | **5,524** |
| MCP tools exposed | **31** |
| Semantic query time | **~256ms** |
| Vector DB size | **493MB** (was 14GB before LanceDB migration) |

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MCP BRAIN SERVER                            â”‚
â”‚              31 tools Â· Claude Code / Desktop                   â”‚
â”‚  semantic_search Â· thinking_trajectory Â· alignment_check Â· ...  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    LANCEDB VECTORS   â”‚  â”‚         DUCKDB + PARQUET             â”‚
â”‚  118K embeddings     â”‚  â”‚  376K messages Â· keyword search      â”‚
â”‚  768d nomic-v1.5     â”‚  â”‚  columnar Â· compressed Â· portable    â”‚
â”‚  493MB on disk       â”‚  â”‚  serverless SQL analytics            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                         â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DATA SOURCES (Immutable)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Claude    â”‚ â”‚ ChatGPT   â”‚ â”‚ Gemini    â”‚ â”‚ Clawdbot  â”‚      â”‚
â”‚  â”‚ Code/     â”‚ â”‚ export    â”‚ â”‚ sessions  â”‚ â”‚ sessions  â”‚      â”‚
â”‚  â”‚ Desktop   â”‚ â”‚           â”‚ â”‚           â”‚ â”‚           â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ GitHub    â”‚ â”‚ Markdown  â”‚ â”‚ Interpretation layers  â”‚        â”‚
â”‚  â”‚ 2.2K      â”‚ â”‚ 5.5K docs â”‚ â”‚ focus Â· mood Â· themes  â”‚        â”‚
â”‚  â”‚ commits   â”‚ â”‚           â”‚ â”‚ spend Â· velocity Â· ...  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AUTO-SYNC PIPELINE                           â”‚
â”‚  Claude Code hook â†’ sync.py â†’ parquet â†’ embed â†’ ready          â”‚
â”‚  Hourly: clawdbot sessions Â· Nightly: all sources + vectors    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Design Decisions

### Facts vs Interpretations

Raw data is **immutable**. Derived analysis lives in versioned layers. Wrong interpretation? Delete the version and rebuild. Source data stays clean forever.

```
data/
â”œâ”€â”€ facts/          # NEVER modified â€” append only
â”‚   â”œâ”€â”€ brain/      # L0 index â†’ L1 summary â†’ L2 content â†’ L3 raw
â”‚   â”œâ”€â”€ spend/      # raw â†’ daily â†’ monthly aggregation
â”‚   â””â”€â”€ sources/    # original parquets (symlinks)
â””â”€â”€ interpretations/ # DERIVED â€” versioned, rebuildable
    â”œâ”€â”€ focus/v1/
    â”œâ”€â”€ mood_patterns/
    â””â”€â”€ weekly_summaries/
```

### Onion Skin Layers (L0â€“L3)

Progressive disclosure â€” query only what you need:

| Layer | Contents | Use Case |
|-------|----------|----------|
| **L0** index | Event IDs, timestamps, source | Quick lookups, counts |
| **L1** summary | 500-char preview + embedding | Semantic search, browsing |
| **L2** content | Full text, has_code, has_url | Deep reading |
| **L3** deep | Symlinks to original parquets | Source verification |

### LanceDB over DuckDB VSS

Started with DuckDB for vector search. Discovered duplicate HNSW indexes created **46x storage overhead** (14GB for 300MB of data). Migrated to LanceDB: **493MB**, same vectors, native incremental indexing. Same 256ms query time.

## MCP Tools (31)

### ðŸ” Search (7)

| Tool | Description |
|------|-------------|
| `semantic_search` | Vector similarity via LanceDB (768d nomic embeddings) |
| `search_conversations` | Keyword search via DuckDB SQL on parquet |
| `unified_search` | Cross-source: conversations + GitHub + markdown |
| `search_ip_docs` | Vector search on curated IP documents |
| `search_markdown` | Keyword search on 5.5K harvested markdown docs |
| `code_to_conversation` | Semantic search across commits + conversations |
| `find_user_questions` | Recent questions asked |

### ðŸ§  Synthesis (4)

| Tool | Description |
|------|-------------|
| `what_do_i_think` | Synthesize your views on any topic from all evidence |
| `find_precedent` | Find similar situations from the past |
| `alignment_check` | Check if a decision aligns with your principles |
| `thinking_trajectory` | Track how an idea evolved over months/years |

### ðŸ’¬ Conversation (5)

| Tool | Description |
|------|-------------|
| `get_conversation` | Full conversation by ID |
| `conversations_by_date` | What happened on a specific date |
| `what_was_i_thinking` | Month snapshot: themes, activity, concepts |
| `concept_velocity` | How often a term appears over time |
| `first_mention` | When a concept first appeared in your history |

### ðŸ™ GitHub (4)

| Tool | Description |
|------|-------------|
| `github_project_timeline` | Repo creation, commits, activity windows |
| `conversation_project_context` | Conversations mentioning a project |
| `validate_date_with_github` | Verify conversation dates via commit timestamps |
| `code_to_conversation` | Bridge code changes to discussion context |

### ðŸ“„ Markdown Corpus (4)

| Tool | Description |
|------|-------------|
| `get_breakthrough_docs` | Documents tagged with high breakthrough energy |
| `get_deep_docs` | High depth-score documents |
| `get_project_docs` | All docs for a specific project |
| `get_open_todos` | Documents with open TODO items |

### ðŸ“Š Analysis (5)

| Tool | Description |
|------|-------------|
| `query_tool_stacks` | Technology stack patterns |
| `query_problem_resolution` | Debugging and problem-solving patterns |
| `query_spend` | Cost breakdown by source and time period |
| `query_timeline` | Cross-source timeline for any date |
| `query_conversation_summary` | Comprehensive conversation analysis |

### âš™ï¸ Meta (2)

| Tool | Description |
|------|-------------|
| `brain_stats` | Overview of all data sources and counts |
| `list_principles` / `get_principle` | Your foundational SEED principles |

## Data Flow

```
  Clawdbot Sessions          Claude Code          ChatGPT Export       Gemini
  ~/.clawdbot/agents/     ~/.claude/projects/     conversations.json   sessions
         â”‚                       â”‚                       â”‚                â”‚
         â–¼                       â–¼                       â–¼                â–¼
     sync_clawdbot.py        live/sync.py          import pipeline    import pipeline
         â”‚                       â”‚                       â”‚                â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â–¼
                            data/all_conversations.parquet (376K messages)
                                                 â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â–¼                         â–¼
                           embed_new_messages.py      build_*.py (88 pipelines)
                                    â”‚                         â”‚
                                    â–¼                         â–¼
                           vectors/brain.lance/       data/interpretations/
                           (118K vectors, 493MB)      (focus, mood, themes, ...)
                                    â”‚                         â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â–¼
                                        mcp_brain_server.py
                                         (31 MCP tools)
                                                 â”‚
                                                 â–¼
                                    Claude Code Â· Claude Desktop
                                      Any MCP-compatible client
```

## Quick Start

### Prerequisites

- Python 3.11+
- Apple Silicon Mac recommended (MPS acceleration for embeddings)
- [mcporter](https://github.com/nicobailey/mcporter) or any MCP client

### 1. Clone & Setup

```bash
git clone https://github.com/mordechaipotash/intellectual-dna.git
cd intellectual-dna

# Create virtual environment
python -m venv mcp-env
source mcp-env/bin/activate

# Install dependencies
pip install duckdb lancedb nomic fastmcp pandas pyarrow
```

### 2. Prepare Your Data

The system expects conversation data in parquet format. Export your conversations:

```bash
# Import ChatGPT export
python -m pipelines import_chatgpt /path/to/conversations.json

# Import Claude Code sessions
python -m pipelines import_claude_code

# Or bring your own parquet with columns: 
# [message_id, conversation_id, role, content, created, source]
```

### 3. Embed & Index

```bash
# Generate embeddings (uses nomic-embed-text-v1.5 locally)
python pipelines/embed_new_messages.py

# Check stats
python pipelines/embed_new_messages.py stats
```

### 4. Run the MCP Server

```bash
# Direct
python mordelab/02-monotropic-prosthetic/mcp_brain_server.py

# Or via mcporter config (~/.mcporter/mcporter.json):
{
  "brain": {
    "command": "python",
    "args": ["mordelab/02-monotropic-prosthetic/mcp_brain_server.py"],
    "lifecycle": "keep-alive"
  }
}
```

### 5. Query Your Brain

```python
# Semantic search
semantic_search("what do I think about productivity?", limit=10)

# Track idea evolution
thinking_trajectory("agency")

# Time-travel to any month
what_was_i_thinking("2024-08")

# Cross-source search
unified_search("database optimization")
```

## Tech Stack

| Component | Choice | Why |
|-----------|--------|-----|
| Vector DB | **LanceDB** | 32x smaller than DuckDB VSS, native incremental, no index footguns |
| Embeddings | **nomic-embed-text-v1.5** | 768d, runs locally on Apple Silicon via MPS |
| Analytics | **DuckDB** | Fast SQL on parquet, serverless, zero config |
| Storage | **Parquet** | Columnar, compressed, portable, ecosystem support |
| Interface | **MCP (FastMCP)** | Direct integration with Claude Code/Desktop |
| Automation | **launchd + hooks** | Native macOS scheduling, zero external deps |
| Pipelines | **88 Python scripts** | Each pipeline is standalone, composable |

## The SEED Principles

Eight foundational mental models extracted from 376K messages:

| Principle | Core Idea |
|-----------|-----------|
| **INVERSION** | Reverse the problem â€” ask what prevents NOT-X |
| **COMPRESSION** | Reduce to essential while preserving decision quality |
| **AGENCY** | 100% human control, 100% machine execution |
| **BOTTLENECK** | Find the constraint, amplify it as leverage |
| **TRANSLATION** | Interface between infinite AI output and finite human comprehension |
| **TEMPORAL** | Human time is the ultimate scarce resource |
| **SEEDS** | Autonomous bounded systems with clear interfaces |
| **COGNITIVE** | Design systems that amplify your brain, not fight it |

## Repository Structure

```
intellectual-dna/
â”œâ”€â”€ mordelab/02-monotropic-prosthetic/
â”‚   â”œâ”€â”€ mcp_brain_server.py          # MCP server (31 tools)
â”‚   â””â”€â”€ SEED-MORDETROPIC-128KB-MASTER.json  # 8 principles
â”œâ”€â”€ pipelines/                        # 88 data pipelines
â”‚   â”œâ”€â”€ embed_new_messages.py         # Parquet â†’ LanceDB vectors
â”‚   â”œâ”€â”€ sync_clawdbot.py             # Clawdbot sessions â†’ parquet
â”‚   â”œâ”€â”€ sync_github.py               # GitHub repos + commits
â”‚   â”œâ”€â”€ harvest_markdown.py          # Markdown corpus builder
â”‚   â”œâ”€â”€ build_*.py                   # 50+ interpretation builders
â”‚   â””â”€â”€ rebuild.py                   # Unified orchestrator
â”œâ”€â”€ live/
â”‚   â”œâ”€â”€ sync.py                      # Auto-sync from Claude Code
â”‚   â””â”€â”€ daily_briefing.py            # Morning briefing agent
â”œâ”€â”€ data/                             # (gitignored)
â”‚   â”œâ”€â”€ facts/                        # Immutable source data
â”‚   â”‚   â””â”€â”€ brain/                    # L0-L3 onion layers
â”‚   â””â”€â”€ interpretations/              # Derived, versioned analysis
â”œâ”€â”€ vectors/                          # (gitignored)
â”‚   â””â”€â”€ brain.lance/                  # 118K vectors (493MB)
â”œâ”€â”€ config.py                         # Central configuration
â””â”€â”€ .claude/CLAUDE.md                 # Context engineering for Claude Code
```

## Lessons Learned

1. **DuckDB VSS has footguns** â€” Accidentally created duplicate HNSW indexes. 14GB for 300MB of data. LanceDB just works.

2. **Facts vs Interpretations prevents rebuild nightmares** â€” Mixing raw data with derived analysis creates cascading corruption. Keep them separate.

3. **Auto-sync beats manual export** â€” Claude Code stop hook triggers `sync.py`. New conversations flow in automatically. Zero friction = actually gets used.

4. **Embeddings beat keywords** â€” "What was I thinking about agency?" finds relevant messages even when you never used that exact word.

5. **88 pipelines > 1 monolith** â€” Each pipeline is a standalone script. Easy to run, debug, or replace individually.

## Related Projects

- **[brain-canvas](https://github.com/mordechaipotash/brain-canvas)** â€” Give any LLM its own display (`npx brain-canvas`)
- **[youtube-transcription-pipeline](https://github.com/mordechaipotash/youtube-transcription-pipeline)** â€” 31K+ videos, transcribed
- **[seedgarden](https://github.com/mordechaipotash/seedgarden)** â€” The SHELET Protocol for AI-human interfaces

## Work With Me

Open to async contract work in context engineering, MCP server development, and AI orchestration systems.

[GitHub](https://github.com/mordechaipotash) Â· [Reddit](https://reddit.com/u/Signal_Usual8630)

---

*Built by [Mordechai Potash](https://github.com/mordechaipotash) â€” a monotropic polymath who needed a system that works with deep focus, not against it.*
